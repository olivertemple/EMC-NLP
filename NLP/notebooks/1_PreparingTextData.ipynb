{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello again, welcome to notebook 1!\n",
    "\n",
    "In this notebook we will look at preparing text data.\n",
    "\n",
    "It is recommended that you complete all exercises that are not marked as optional.\n",
    "\n",
    "Feel free to be creative and write your own code wherever you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-48c62f65c5b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1: Preparing text data\n",
    "In order to get a computer to understand a piece of text - whether that's a news headline, a tweet or a Wikipedia article - we need to break it up into components that the computer will recognise. This is called tokenization. We will also want to remove common words like 'and', 'in', 'but', etc to allow the computer to focus on more important words. In NLP, words such as these are known as stopwords.\n",
    "\n",
    "In this lesson we will focus on tokenization and stopword removal for preparing text data. In reality, however, there are many more techniques that we could use to prepare our data, such as lemmatization and stemming. You may want to research and implement these techniques for the final challenge!\n",
    "\n",
    "\n",
    "### Exercise 1.1: Tokenization\n",
    "\n",
    "As described above, the technique of splitting up sentences or documents into smaller components is called tokenization. When we perform tokenization, there are many different things we must consider. Should we split up the text into characters, or words? What should we do about punctuation? In this exercise we will think about some of these issues and build our own basic tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-486b9b759c39>, line 6)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-486b9b759c39>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    possibility_1 = # TODO\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Q1.1.1 - Think of three different ways in which you could tokenize the below sentence\n",
    "#        - Which of the tokenizations will be the best, do you think? Why?\n",
    "\n",
    "animal_sentence = \"Cats, in my opinion, aren't better than dogs.\"\n",
    "\n",
    "possibility_1 = #cats   in my opinion   arent better than dogs\n",
    "possibility_2 = # TODO\n",
    "possibility_3 = # TODO\n",
    "\n",
    "# Q1.1.2 - What problems could we encounter when trying to tokenize a dataset of tweets? \n",
    "#        - How might we overcome these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully these questions have emphasised that there are lots of things to think about when we perform tokenization, and there is no single 'best' method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"Cats, in my opinion, aren't better than dogs.\", 'Dogs are the best!']\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ollie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-fe740c465b95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mspacy_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# Q1.1.3 (optional) - Try importing some tokenizers from different Python modules \n",
    "#                   - e.g. nltk, torchtext, spacy\n",
    "#                   - Compare their outputs, do they all tokenize sentences in the same way?\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentence = \"Cats, in my opinion, aren't better than dogs. Dogs are the best!\"\n",
    "tokenized_sentence = sent_tokenize(sentence)\n",
    "print(tokenized_sentence)\n",
    "\n",
    "###i couldnt get spacy to install\n",
    "#import spacy\n",
    "#spacy_en = spacy.load('en')\n",
    "#def tokenizer(text):\n",
    "#    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "#print(tokenizer(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll concentrate on just a handful of points when we tokenize our sentences:\n",
    "- Splitting sentences into a list of words.\n",
    "- Removing punctuation.\n",
    "- Changing any uppercase letters to lowercase.\n",
    "\n",
    "For Q1.1.4 you must complete the skeleton code for a ```basic_tokenizer```.\n",
    "There are a few hints below that you may want to use in your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['cats', 'in', 'my', 'opinion', 'arent', 'better', 'than', 'dogs']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# Q1.1.4\n",
    "import string as st\n",
    "punctuation = st.punctuation\n",
    "def basic_tokenizer(sentence):\n",
    "    string = sentence.lower()\n",
    "    newstring = ''\n",
    "    for char in string:\n",
    "        if char not in punctuation:\n",
    "            newstring += char\n",
    "    stingOfWord = newstring.split(' ')\n",
    "    return(stingOfWord)\n",
    "\n",
    "basic_tokenizer(\"Cats, in my opinion, aren't better than dogs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hint 1: ['This', 'is', 'hint', '1']\nHint 2: This Is Hint 2\nHint 3: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\nHint 4: this is your final hint\n"
     ]
    }
   ],
   "source": [
    "hint1 = 'This is hint 1'.split()\n",
    "print('Hint 1:', hint1)\n",
    "\n",
    "hint2 = 'This! Is! Hint! 2!'.replace('!', '')\n",
    "print('Hint 2:', hint2)\n",
    "\n",
    "hint3 = string.punctuation\n",
    "print('Hint 3:', hint3)\n",
    "\n",
    "hint4 = 'THIS IS YOUR FINAL HINT'.lower()\n",
    "print('Hint 4:', hint4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your ```basic_tokenizer``` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your function passed the test!\n"
     ]
    }
   ],
   "source": [
    "test1 = \"Cats, in my opinion, aren't better than dogs\"\n",
    "if sorted(basic_tokenizer(test1)) == ['arent', 'better', 'cats', 'dogs', 'in', 'my', 'opinion', 'than']:\n",
    "    print('Your function passed the test!')\n",
    "else:\n",
    "    print(\"Oops, your function isn't working quite yet. Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Stopwords\n",
    "\n",
    "In NLP, stopwords are common words that do not add much value to the meaning of a sentence or document. In English, these include: 'the', 'is', 'in', 'for', 'at', etc. We often want to remove these from our data for NLP problems. For example, consider the following two sentences:\n",
    "\n",
    "1. There is a cat in this hat.\n",
    "2. There is a flaw in this argument.\n",
    "\n",
    "We can tell that these sentences are about totally different things, despite having 5 of 7 words in common, but computers aren't as good at infering meaning as we are!\n",
    "\n",
    "In English, there are a lot of words that we could consider stopwords. Thankfully, there are Python libraries that contain stopword lists that we can use, rather than creating our own lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "179\nyourselves\n"
     ]
    }
   ],
   "source": [
    "# Q1.2.1 - How many stopwords are there in this list?\n",
    "#        - Which stopword is the longest?\n",
    "\n",
    "print(len(stop_words))\n",
    "print(max(stop_words, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of stopwords, we need a function to remove them from our text. \n",
    "\n",
    "For Q2.2 you must complete the function ```remove_stopwords``` below.\n",
    "\n",
    "The function should take as input a list ```sentence``` of words and a list ```stop_words``` of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['test']"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "# Q1.2.2 \n",
    "\n",
    "def remove_stopwords(list, stop_words):\n",
    "    output = []\n",
    "    for item in list:\n",
    "        if item not in stop_words:\n",
    "            output.append(item)\n",
    "    return output\n",
    "\n",
    "remove_stopwords([\"this\",\"is\",\"a\",\"test\"], stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your ```remove_stopwords``` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your function passed the test!\n"
     ]
    }
   ],
   "source": [
    "test2 = ['there', 'is', 'a', 'cat', 'in', 'this', 'hat']\n",
    "if sorted(remove_stopwords(test2, stop_words)) == ['cat', 'hat']:\n",
    "    print('Your function passed the test!')\n",
    "else:\n",
    "    print(\"Oops, your function didn't remove all of the stopwords. Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Final processing\n",
    "\n",
    "Now that we've written functions to tokenize sentences and to remove stopwords, we want to combine them into a complete preprocessing function.\n",
    "\n",
    "* Q1.3.1 - Write a function ```preprocess``` that preprocesses a sentence ready for NLP techniques to be applied. Your function should tokenize an input sentence and remove stopwords from a predefined list.\n",
    "\n",
    "* Q1.3.2 (optional) - Research lemmatization and stemming in relation to NLP. Add these techniques to your ```preprocess``` function. (**HINT**: Are there any Python libraries that you could use to help you?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.3.1 / Q1.3.2\n",
    "\n",
    "def preprocess(sentence, stop_words):\n",
    "    sentence = basic_tokenizer(sentence)\n",
    "    clean_sentence = remove_stopwords(sentence, stop_words)\n",
    "\n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your ```preprocess``` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Congratulations! Your preprocessor is working and ready to go.\n"
     ]
    }
   ],
   "source": [
    "test3 = 'This is the final test! What will the result be?'\n",
    "if sorted(preprocess(test3, stop_words)) == ['final', 'result', 'test']:\n",
    "    print('Congratulations! Your preprocessor is working and ready to go.')\n",
    "else:\n",
    "    print('Not quite! Try again.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "343de2cd4032818e6bd6da839f7e326daa435e2ee81af0844da206feccdd3ad9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}