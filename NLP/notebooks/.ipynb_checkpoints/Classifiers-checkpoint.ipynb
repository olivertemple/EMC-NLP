{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hey there, welcome to your final challenge\n",
    "\n",
    "You're finally going to have a chance to apply the skills you've been learning on a real-life dataset.\n",
    "\n",
    "Your task is to predict if a given news headline is sarcastic or not.\n",
    "\n",
    "<img src=\"data:image/jpeg;base64,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\" width=400>\n",
    "\n",
    "--- \n",
    "\n",
    "A good way to think about how a machine might detect sarcasm is to try it yourself.\n",
    "Which of these headlines is sarcastic?\n",
    "\n",
    "* \"Thinking about the way you look all the time burns 5,000 calories an hour\"\n",
    "* \"Safeguarding the well-being of children\"\n",
    "\n",
    "How did you make your decision? Which features of the text gave it away?\n",
    "This is the thought process you will need to make a good model.\n",
    "\n",
    "--- \n",
    "\n",
    "Feel free to be creative and write your own code wherever you want!\n",
    "\n",
    "The provided functions are only there to help you if you get stuck :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data: 25862\n",
      "Sarcasm percentage: 43%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the in the data from CSV\n",
    "df = pd.read_csv('../data/clean_df.csv')\n",
    "\n",
    "# Print the number of headlines that we're dealing with\n",
    "print(f'Size of data: {len(df)}')\n",
    "\n",
    "# Print the percentage of headlines that are sarcastic\n",
    "sarcasm_percentage = int(100 * sum(df['is_sarcastic']) / len(df))\n",
    "print(f'Sarcasm percentage: {sarcasm_percentage}%')\n",
    "\n",
    "# Show a sample of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test\n",
    "\n",
    "There are two parts to building a model.\n",
    "\n",
    "1) Train the model on train data\n",
    "\n",
    "2) Test the performance of the model on test data\n",
    "\n",
    "Is is very important that the train and test data are distinct!\n",
    "\n",
    "A model may look good due to its performance on the training data,\n",
    "but be bad because of its inability to generalize to test data.\n",
    "This is a concept called *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(df, train_fraction=0.9):\n",
    "    \"\"\"\n",
    "    Split the corpus and labels into train and test data sets\n",
    "    \n",
    "    :param df: pd.core.frame.DataFrame\n",
    "             : The sarcasm data\n",
    "    :param train_fraction: float (default = 0.9)\n",
    "                         : The fraction of data to use for training\n",
    "    \"\"\"\n",
    "    # Get the headline and sarcasm columns\n",
    "    X = df['headline'].to_numpy()\n",
    "    y = df['is_sarcastic'].to_numpy()\n",
    "    \n",
    "    # Use a fraction of the data for training, leaving the remaining data for testing\n",
    "    n_train = int(len(df) * train_fraction)\n",
    "    X_train = X[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_train = y[:n_train]\n",
    "    y_test = y[n_train:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_and_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature creation\n",
    "\n",
    "Now that we have our data, we need to create features to feed into our model.\n",
    "\n",
    "Below we're going to extract keywords of interest from the headlines, but using your knowledge from prior notebooks you should do something more sophisticated!\n",
    "\n",
    "What other features of the headline might be important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rewrite this function to return sophisticated features\n",
    "def create_features(X_train, X_test, vocab):\n",
    "    \n",
    "    # Convert our headlines into bag-of-words\n",
    "    X_train = [[headline.count(w) for w in vocab] for headline in X_train]\n",
    "    X_test = [[headline.count(w) for w in vocab] for headline in X_test]\n",
    "    \n",
    "    # Normalize the data so that it lies in the interval [0, 1]\n",
    "    # This will be useful for checking the performance of our model later on\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    \n",
    "    return X_train, X_test\n",
    "    \n",
    "vocab = ['trump', 'nation', 'area', 'onion']\n",
    "X_train, X_test = create_features(X_train, X_test, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Suppose we have features $x_1, x_2, ..., x_n$ with a target output $y$.\n",
    "\n",
    "For our use case the $x_i$ are word counts, and $y$ is whether the headline is sarcastic or not.\n",
    "\n",
    "---\n",
    "\n",
    "In school you will have learned about *linear regression*,\n",
    "where the goal is to learn coefficients $a_1, a_2, ..., a_n$\n",
    "that minimize the error\n",
    "\n",
    "$err = \\sqrt {(y_* - y)^2}$\n",
    "\n",
    "where $y_* = a_1 x_1 + a_2 x_2 + ... + a_n x_n$\n",
    "\n",
    "This is a good method when then $y_*$ are unbounded, \n",
    "but we need our outputs to be probabilities,\n",
    "so they have to lie in the closed interval $[0, 1]$.\n",
    "\n",
    "---\n",
    "\n",
    "We can achieve this with *logistic regression*.\n",
    "\n",
    "This is an adapation of linear regression where we push $y_*$ through a *logistic function*\n",
    "\n",
    "$\\sigma(y_*) = \\frac{1}{1 + e^{-y_*}}$\n",
    "\n",
    "Exercise: Check that for any real-valued input $y_*, \\sigma(y_*)$ lies in $[0, 1]$.\n",
    "\n",
    "---\n",
    "\n",
    "We will be borrowing an implementation of logistic regression from Sklearn.\n",
    "\n",
    "It gives us a baseline of about 60% accuracy on the test set.\n",
    "\n",
    "By changing the features you should be able to achieve over 90% accuracy, we believe in you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 59%\n",
      "Test accuracy: 60%\n"
     ]
    }
   ],
   "source": [
    "def fit_model_and_print_scores(X_train, X_test, y_train, y_test):\n",
    "    # Train a logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the accuracy of the model on the train and the test data\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    print(f'Train accuracy: {int(100*train_score)}%')\n",
    "    print(f'Test accuracy: {int(100*test_score)}%')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = fit_model_and_print_scores(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "\n",
    "To improve our model we'll need to examine where it's going wrong.\n",
    "\n",
    "Let's have a look at some of the headlines where it's failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 2\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: a game show winner on his biggest jackpot yet: a son\n",
      "\n",
      "Idx: 3\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: how to actually get a bartender's attention\n",
      "\n",
      "Idx: 5\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: former priest convicted in decades-old beauty queen slaying\n",
      "\n",
      "Idx: 7\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: 5 things i wish i'd been told on my wedding day\n",
      "\n",
      "Idx: 8\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: 'steven universe' is exploring unhealthy relationships for a young, queer audience\n",
      "\n",
      "Idx: 10\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: 9 planet-happy trips to book on earth day\n",
      "\n",
      "Idx: 13\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: rex tillerson supposedly shifted exxon mobil's climate position. except he really didn't.\n",
      "\n",
      "Idx: 15\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: hilarious video shows 'how to bake easter cookies like a toddler'\n",
      "\n",
      "Idx: 16\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: multiple tornadoes rip through oklahoma, injuring 7\n",
      "\n",
      "Idx: 18\n",
      "Prediction: 1\n",
      "Actual: 0\n",
      "Headline: 'how to get away with murder's' heroine gets more complex\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def debug_output(y_pred, y_test, df, n_samples=5):\n",
    "    \"\"\"\n",
    "    Debug the output of your model\n",
    "    \n",
    "    :param y_pred: np.ndarray[bool]\n",
    "                 : Predictions of the sarcasm of X_test\n",
    "    :param y_test: np.ndarray[bool]\n",
    "                 : The true sarcasm of X_test\n",
    "    :param df: pd.core.frame.DataFrame\n",
    "             : The sarcasm data\n",
    "    :param n_samples: int (default = 5)\n",
    "                    : The number of samples to debug\n",
    "    \"\"\"\n",
    "    n_train = len(df) - len(y_pred)\n",
    "    test_headlines = df['headline'].to_numpy()[n_train:]\n",
    "    \n",
    "    # Find the predictions which were wrong\n",
    "    failed_idx = np.where(y_pred != y_test)[0]\n",
    "    \n",
    "    # Display a sample of the headlines where our model failed\n",
    "    for idx in failed_idx[:n_samples]:\n",
    "        print(f'Idx: {idx}')\n",
    "        print(f'Prediction: {y_pred[idx]}')\n",
    "        print(f'Actual: {y_test[idx]}')\n",
    "        print(f'Headline: {test_headlines[idx]}')\n",
    "        print('')\n",
    "    \n",
    "y_pred = model.predict(X_test)\n",
    "debug_output(y_pred, y_test, df, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further debugging\n",
    "\n",
    "To improve our model it is important to examine how it's making predictions.\n",
    "We can do this by looking at the importance that it places on each word.\n",
    "\n",
    "If the coefficient is positive, the word is used mostly in sarcastic headlines.\n",
    "\n",
    "If the coefficient is negative, the word is used mostly in non-sarcastic headlines.\n",
    "\n",
    "---\n",
    "\n",
    "Could this be useful for detecting which features are important?\n",
    "\n",
    "Could this be useful for detecting overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area: 3.30\n",
      "onion: 1.98\n",
      "trump: -1.68\n",
      "nation: 1.11\n"
     ]
    }
   ],
   "source": [
    "def print_important_words(coef, vocab, n_words=10):\n",
    "    \"\"\"\n",
    "    Print the most important words according to the model coefficients\n",
    "    \n",
    "    :param coef: list[float]\n",
    "               : The importance of each coefficient\n",
    "    :param vocab: list[str]\n",
    "                : The vocabulary corresponding to the coefficients\n",
    "    :param n_words: int (default = 10)\n",
    "                  : The number of top words to show\n",
    "    \"\"\"\n",
    "    # Put the coefficients in order from biggest magnitude to smallest magnitude\n",
    "    top_idx = np.argsort(-abs(coef))\n",
    "    \n",
    "    # Take the top `n_words` coefficients\n",
    "    top_idx = top_idx[:n_words]\n",
    "    \n",
    "    for i in top_idx:\n",
    "        print(f'{vocab[i]}: {coef[i]:.2f}')\n",
    "\n",
    "coef = model.coef_[0]\n",
    "print_important_words(coef, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline examination\n",
    "\n",
    "If we're interested in a particular word, we can print the headlines that contain it.\n",
    "\n",
    "This will help us to select new features and gain a better understanding of the current features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n",
      "Headline: son of edward r. murrow says father 'real dirtbag' compared to onion reporters\n",
      "\n",
      "Label: 1\n",
      "Headline: heroic police officer talks man down from edge of purchasing subway footlong sweet onion chicken teriyaki\n",
      "\n",
      "Label: 1\n",
      "Headline: the onion apologizes\n",
      "\n",
      "Label: 1\n",
      "Headline: obama, romney urge americans to purchase 'the onion book of known knowledge'\n",
      "\n",
      "Label: 0\n",
      "Headline: the onion is getting into the movie business\n",
      "\n",
      "Label: 1\n",
      "Headline: onion twitter password changed to onionman77\n",
      "\n",
      "Label: 1\n",
      "Headline: fabled burger king employee places single onion ring in everyone's fries\n",
      "\n",
      "Label: 1\n",
      "Headline: mother of slaying victim glad it was onion reporter who knocked on her door half an hour after funeral\n",
      "\n",
      "Label: 1\n",
      "Headline: whales beach selves in attempt to purchase 'the onion book of known knowledge'\n",
      "\n",
      "Label: 1\n",
      "Headline: 'arby's has been putting more onion bits on their buns,' reports man sinking into heavy depression\n",
      "\n",
      "Label: 1\n",
      "Headline: man regrets straying from sour cream and onion potato chips\n",
      "\n",
      "Sarcasm: 10\n",
      "Not sarcasm: 1\n"
     ]
    }
   ],
   "source": [
    "def print_headlines_with_word(df, word):\n",
    "    \"\"\"\n",
    "    Find the headlines which contain the given word\n",
    "    \n",
    "    :param df: pd.core.frame.DataFrame\n",
    "             : The sarcasm data\n",
    "    :param word: str\n",
    "               : The word of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    sarcasm = 0\n",
    "    not_sarcasm = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        # Check if the word is in the headline\n",
    "        if word in row['headline'].split(' '):\n",
    "            \n",
    "            print(f'Label: {row[\"is_sarcastic\"]}')\n",
    "            print(f'Headline: {row[\"headline\"]}\\n')\n",
    "            \n",
    "            if row['is_sarcastic']:\n",
    "                sarcasm += 1\n",
    "            else:\n",
    "                not_sarcasm += 1\n",
    "                \n",
    "    print(f'Sarcasm: {sarcasm}')\n",
    "    print(f'Not sarcasm: {not_sarcasm}')\n",
    "            \n",
    "print_headlines_with_word(df, 'onion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
