{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello again, welcome to another notebook!\n",
    "\n",
    "In this notebook we will look at preparing text data.\n",
    "\n",
    "It is recommended that you complete all exercises that are not marked as optional.\n",
    "\n",
    "Feel free to be creative and write your own code wherever you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/x1908440/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1: Preparing text data\n",
    "In order to get a computer to understand a piece of text - whether that's a news headline, a tweet or a Wikipedia article - we need to break it up into components that the computer will recognise. This is called tokenization. We will also want to remove common words like 'and', 'in', 'but', etc to allow the computer to focus on more important words. In NLP, words such as these are known as stopwords.\n",
    "\n",
    "In this lesson we will focus on tokenization and stopword removal for preparing text data. In reality, however, there are many more techniques that we could use to prepare our data, such as lemmatization and stemming. You may want to research and implement these techniques for the final challenge!\n",
    "\n",
    "\n",
    "### Exercise 1.1: Tokenization\n",
    "\n",
    "As described above, the technique of splitting up sentences or documents into smaller components is called tokenization. When we perform tokenization, there are many different things we must consider. Should we split up the text into characters, or words? What should we do about punctuation? In this exercise we will think about some of these issues and build our own basic tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.1.1 - Think of three different ways in which you could tokenize the below sentence\n",
    "#        - Which of the tokenizations will be the best, do you think? Why?\n",
    "\n",
    "animal_sentence = \"Cats, in my opinion, aren't better than dogs.\"\n",
    "\n",
    "possibility_1 = # TODO\n",
    "possibility_2 = # TODO\n",
    "possibility_3 = # TODO\n",
    "\n",
    "# Q1.1.2 - What problems could we encounter when trying to tokenize a dataset of tweets? \n",
    "#        - How might we overcome these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully these questions have emphasised that there are lots of things to think about when we perform tokenization, and there is no single 'best' method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.1.3 (optional) - Try importing some tokenizers from different Python modules \n",
    "#                   - e.g. nltk, torchtext, spacy\n",
    "#                   - Compare their outputs, do they all tokenize sentences in the same way?\n",
    "\n",
    "import # TODO\n",
    "\n",
    "sentence = # TODO\n",
    "tokenized_sentence = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll concentrate on just a handful of points when we tokenize our sentences:\n",
    "- Splitting sentences into a list of words.\n",
    "- Removing punctuation.\n",
    "- Changing any uppercase letters to lowercase.\n",
    "\n",
    "For Q1.1.4 you must complete the skeleton code for a ```basic_tokenizer```.\n",
    "There are a few hints below that you may want to use in your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.1.4\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    # TODO: Create a tokenizer\n",
    "    # Solution to be removed\n",
    "    out = sentence\n",
    "    for x in string.punctuation:\n",
    "        out = out.replace(x, '')\n",
    "    out = out.lower().split()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hint 1: ['This', 'is', 'hint', '1']\n",
      "Hint 2: This Is Hint 2\n",
      "Hint 3: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Hint 4: this is your final hint\n"
     ]
    }
   ],
   "source": [
    "hint1 = 'This is hint 1'.split()\n",
    "print('Hint 1:', hint1)\n",
    "\n",
    "hint2 = 'This! Is! Hint! 2!'.replace('!', '')\n",
    "print('Hint 2:', hint2)\n",
    "\n",
    "hint3 = string.punctuation\n",
    "print('Hint 3:', hint3)\n",
    "\n",
    "hint4 = 'THIS IS YOUR FINAL HINT'.lower()\n",
    "print('Hint 4:', hint4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your ```basic_tokenizer``` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your function passed the test!\n"
     ]
    }
   ],
   "source": [
    "test1 = \"Cats, in my opinion, aren't better than dogs\"\n",
    "if sorted(basic_tokenizer(test1)) == ['arent', 'better', 'cats', 'dogs', 'in', 'my', 'opinion', 'than']:\n",
    "    print('Your function passed the test!')\n",
    "else:\n",
    "    print(\"Oops, your function isn't working quite yet. Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Stopwords\n",
    "\n",
    "In NLP, stopwords are common words that do not add much value to the meaning of a sentence or document. In English, these include: 'the', 'is', 'in', 'for', 'at', etc. We often want to remove these from our data for NLP problems. For example, consider the following two sentences:\n",
    "\n",
    "1. There is a cat in this hat.\n",
    "2. There is a flaw in this argument.\n",
    "\n",
    "We can tell that these sentences are about totally different things, despite having 5 of 7 words in common, but computers aren't as good at infering meaning as we are!\n",
    "\n",
    "In English, there are a lot of words that we could consider stopwords. Thankfully, there are Python libraries that contain stopword lists that we can use, rather than creating our own lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.2.1 - How many stopwords are there in this list?\n",
    "#        - Which stopword is the longest?\n",
    "\n",
    "# TODO - Manipulate stopwords to answer the above questions\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of stopwords, we need a function to remove them from our text. \n",
    "\n",
    "For Q2.2 you must complete the function ```remove_stopwords``` below.\n",
    "\n",
    "The function should take as input a list ```sentence``` of words and a list ```stop_words``` of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.2.2 \n",
    "\n",
    "def remove_stopwords(sentence, stop_words):\n",
    "    \"\"\"\n",
    "    :param sentence: list\n",
    "                   : A list of words\n",
    "    :param stop_words: list\n",
    "                     : A list of stopwords\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function\n",
    "    # Solution to be removed\n",
    "    return [word for word in sentence if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your ```remove_stopwords``` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your function passed the test!\n"
     ]
    }
   ],
   "source": [
    "test2 = ['there', 'is', 'a', 'cat', 'in', 'this', 'hat']\n",
    "if sorted(remove_stopwords(test2, stop_words)) == ['cat', 'hat']:\n",
    "    print('Your function passed the test!')\n",
    "else:\n",
    "    print(\"Oops, your function didn't remove all of the stopwords. Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Final processing\n",
    "\n",
    "Now that we've written functions to tokenize sentences and to remove stopwords, we want to combine them into a complete preprocessing function.\n",
    "\n",
    "* Q1.3.1 - Write a function ```preprocess``` that preprocesses a sentence ready for NLP techniques to be applied. Your function should tokenize an input sentence and remove stopwords from a predefined list.\n",
    "\n",
    "* Q1.3.2 (optional) - Research lemmatization and stemming in relation to NLP. Add these techniques to your ```preprocess``` function. (**HINT**: Are there any Python libraries that you could use to help you?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.3.1 / Q1.3.2\n",
    "\n",
    "def preprocess(sentence, stop_words):\n",
    "    \"\"\"\n",
    "    :param sentence: list\n",
    "                   : A list of words\n",
    "    :param stop_words: list\n",
    "                     : A list of stopwords\n",
    "    :return: clean_sentence: list\n",
    "                           : A tokenized sentence with stopwords removed (and more?!)\n",
    "    \"\"\"\n",
    "    # TO DO: Complete this\n",
    "    # Solution to be removed\n",
    "    clean_sentence = remove_stopwords(basic_tokenizer(sentence), stop_words)\n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your ```preprocess``` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! Your preprocessor is working and ready to go.\n"
     ]
    }
   ],
   "source": [
    "test3 = 'This is the final test! What will the result be?'\n",
    "if sorted(preprocess(test3, stop_words)) == ['final', 'result', 'test']:\n",
    "    print('Congratulations! Your preprocessor is working and ready to go.')\n",
    "else:\n",
    "    print('Not quite! Try again.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
