{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMCNLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB8YHxYkb99j"
      },
      "source": [
        "# EMC NLP - Oliver Temple\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "## Stop Words\r\n",
        "The aim of this task was to calculate the bounds on the percentage of the corpus that was removed, when the 10 most common words were removed.\r\n",
        "\r\n",
        "The size of the corpus was $10^5$, and we were given that frequency of the kth most popular word was $1/kH_{N}$, where $log(N)\\le H_{N}\\le log(N)+1$\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_G-Q3hMvi32",
        "outputId": "febd887c-32e7-48be-8c04-83d928d76168"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "def removeStopWordPercent(N,k):\r\n",
        "    sumlower=0\r\n",
        "    sumupper=0\r\n",
        "    for n in range(1,k+1):\r\n",
        "        sumlower+= 1/(n*math.log10(N)+n)\r\n",
        "        sumupper+=1/(n*math.log10(N))\r\n",
        "    return (sumlower,sumupper)\r\n",
        "\r\n",
        "N = 10e4\r\n",
        "k=10\r\n",
        "\r\n",
        "sumlower,sumupper = removeStopWordPercent(N,k)\r\n",
        "print(str((sumlower/N)*100)+\"% ≤ freq ≤ \"+str((sumupper/N)*100)+\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}